{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Random Forest**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "food_train=pd.read_csv('data/food_train.csv')\n",
    "food_test=pd.read_csv('data/food_test.csv')\n",
    "nutri=pd.read_csv('data/nutrients.csv')\n",
    "food_nutri=pd.read_csv('data/food_nutrients.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "nutrients = pd.merge(food_nutri, nutri, how='left',on='nutrient_id') #merge the nutrients with our food description\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "food_train_w_nut=pd.merge(food_train, nutrients, how='left', on='idx') #merge the nutrients data with our food train data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "categories=list(food_train_w_nut['category'].drop_duplicates())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "\n",
    "popcorn_peanuts_keywords = \"|\".join([\"cashews\", \"popcorn\", \"seeds\", \"nuts\", \"macadamias\", \"peanuts\",\"roasted\", \"pistachios\"])\n",
    "candy_keywords = \"|\".join([\"gummi\",\"chewy\", \"gummy\", \"lolli\", \"candy\", \"candies\", \"fruit\", \"licorice\", \"drops\", \"peeps\", \"jelly\", \"fizz\"])\n",
    "cookies_keywords = \"|\".join([\"cookies\", \"sandwich\",\"wafers\"])\n",
    "chips_keywords = \"|\".join([\"chips\", \"pretzel\", \"tortilla\"])\n",
    "chocolate_keywords = \"|\".join([\"truffles\", \"bar\"])\n",
    "cakes_keywords = \"|\".join([\"cake\", \"brownie\", \"pie\", \"eclair\", \"donut\"])\n",
    "\n",
    "def predict_category(description):\n",
    "    if re.search(popcorn_peanuts_keywords, description):\n",
    "        return \"popcorn_peanuts_seeds_related_snacks\"\n",
    "    if re.search(candy_keywords, description):\n",
    "        return \"candy\"\n",
    "    if re.search(cookies_keywords, description):\n",
    "        return \"cookies_biscuits\"\n",
    "    if re.search(chips_keywords, description):\n",
    "        return \"chips_pretzels_snacks\"\n",
    "    if re.search(chocolate_keywords, description):\n",
    "        return \"chocolate\"\n",
    "    if re.search(cakes_keywords, description):\n",
    "        return \"cakes_cupcakes_snack_cakes\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "food_train['predicted_category'] = food_train['description'].apply(predict_category)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "popcorn_peanuts_key_brand = \"|\".join(['star snacks co., inc.', \"nabisco food company\",'john b. sanfilippo & son, inc.','kar nut products company','ferris coffee & nut co.'])\n",
    "candy_key_brand = \"|\".join(['sunmark','wm. wrigley jr. company','perfetti van melle usa inc.','jelly belly candy company'])\n",
    "cookies_key_brand = \"|\".join(['keebler company'])\n",
    "chips_key_brand = \"|\".join([\"the hain celestial group, inc.\"])\n",
    "chocolate_key_brand = \"|\".join([\"lindt & sprungli (schweiz) ag\"])\n",
    "cakes_key_brand = \"|\".join(['hostess brands, llc','tasty baking company'])\n",
    "\n",
    "def predict_category_by_brand(brand):\n",
    "    if brand in popcorn_peanuts_key_brand:\n",
    "        return \"popcorn_peanuts_seeds_related_snacks\"\n",
    "    if brand in candy_key_brand:\n",
    "        return \"candy\"\n",
    "    if brand in cookies_key_brand:\n",
    "        return \"cookies_biscuits\"\n",
    "    if brand in chips_key_brand:\n",
    "        return \"chips_pretzels_snacks\"\n",
    "    if brand in chocolate_key_brand:\n",
    "        return \"chocolate\"\n",
    "    if brand in cakes_key_brand:\n",
    "        return \"cakes_cupcakes_snack_cakes\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "food_train['predicted_category_by_brand'] = food_train['brand'].apply(predict_category_by_brand)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def create_predict_y(row):\n",
    "    if row['predicted_category_by_brand'] == row['predicted_category']:\n",
    "        return row['predicted_category']\n",
    "    elif row['predicted_category_by_brand'] is not None and row['predicted_category'] is not None:\n",
    "        return None\n",
    "    elif row['predicted_category_by_brand'] is not None:\n",
    "        return row['predicted_category_by_brand']\n",
    "    elif row['predicted_category'] is not None:\n",
    "        return row['predicted_category']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "food_train['predict_y'] = food_train.apply(create_predict_y, axis=1)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "unique_characters = set()\n",
    "food_train['ingredients']=food_train['ingredients'].str.replace('[*&$%]', '', regex=True)\n",
    "food_train['ingredients_list']=food_train['ingredients'].apply(lambda x :re.split(r',\\s*(?![^()]*\\))',str(x)))\n",
    "food_train['len_ingredients']=food_train['ingredients_list'].str.len()\n",
    "\n",
    "\n",
    "for ingredient in food_train['ingredients']:\n",
    "    if isinstance(ingredient, str):\n",
    "        unique_characters.update(ingredient)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the code provided below, our goal is to identify all the ingredients that exist within the dataset.\n",
    "Subsequently, we aim to determine the frequency of occurrence for each ingredient throughout the dataset.\n",
    "Following this, we identify the commonly occurring ingredients and generate a new column that indicates the presence of ingredients that intersect with these frequently appearing terms\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "\n",
    "food_test['ingredients_list']=food_test['ingredients'].apply(lambda x :re.split(r',\\s*(?![^()]*\\))',str(x)))\n",
    "\n",
    "food_train_no_ch = food_train.copy()\n",
    "food_test_no_ch= food_test.copy()\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s,%]', '', text)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "food_train_no_ch['ingredients'] = food_train_no_ch['ingredients'].apply(clean_text)\n",
    "food_test_no_ch['ingredients'] = food_test_no_ch['ingredients'].apply(clean_text)\n",
    "\n",
    "\n",
    "ingredient_counts = Counter()\n",
    "\n",
    "for i in food_train_no_ch['ingredients'].str.lower().str.split(', '):\n",
    "    if isinstance(i, list):\n",
    "        for ingredient in i:\n",
    "            if ingredient in ingredient_counts:\n",
    "                ingredient_counts[ingredient] += 1\n",
    "            else:\n",
    "                ingredient_counts[ingredient] = 1\n",
    "\n",
    "\n",
    "common_ingredient = sorted(ingredient_counts.keys(), key=lambda x: ingredient_counts[x], reverse=True)\n",
    "top_common = common_ingredient[:1700]\n",
    "\n",
    "\n",
    "def intersection(list_1, list_2):\n",
    "    temp = set(list_2)\n",
    "    common_list = [value for value in list_1 if value in temp]\n",
    "    return common_list\n",
    "\n",
    "\n",
    "food_train_no_ch['top_ingredients']=food_train_no_ch['ingredients_list'].apply(lambda x:intersection(top_common,x))\n",
    "food_test_no_ch['top_ingredients']=food_test_no_ch['ingredients_list'].apply(lambda x:intersection(top_common,x))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([top_common])\n",
    "\n",
    "\n",
    "train_ing = pd.DataFrame(mlb.transform(food_train_no_ch['top_ingredients'])\n",
    ",\n",
    " columns=mlb.classes_,\n",
    " index=food_train_no_ch.index)\n",
    "\n",
    "test_ing = pd.DataFrame(mlb.transform(food_test_no_ch['top_ingredients'])\n",
    ",\n",
    " columns=mlb.classes_,\n",
    " index=food_test_no_ch.index)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, once again, we retrieve only the words that appear most frequently in the 'household_serving_fulltext' column\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "threshold_percentage = 0.007\n",
    "value_counts = food_train_no_ch['household_serving_fulltext'].value_counts()\n",
    "threshold_frequency = len(food_train_no_ch) * threshold_percentage\n",
    "values_above_threshold = value_counts[value_counts > threshold_frequency].index.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def check_value(value, values_above_threshold):\n",
    "    if value in values_above_threshold:\n",
    "        return value\n",
    "    else:\n",
    "        return 'other'\n",
    "food_train_no_ch['household_serving_common'] = food_train_no_ch['household_serving_fulltext'].apply(lambda x: check_value(x,values_above_threshold))\n",
    "food_test_no_ch['household_serving_common'] = food_test_no_ch['household_serving_fulltext'].apply(lambda x: check_value(x,values_above_threshold))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def hotone(train_col, test_col):\n",
    "    oh = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    trained_oh = oh.fit(train_col.values.reshape(-1, 1))\n",
    "    train_new = trained_oh.transform(train_col.values.reshape(-1, 1))\n",
    "    train_new = pd.DataFrame.sparse.from_spmatrix(train_new, index=train_col.index)\n",
    "\n",
    "    test_new = trained_oh.transform(test_col.values.reshape(-1, 1))\n",
    "    test_new = pd.DataFrame.sparse.from_spmatrix(test_new, index=test_col.index)\n",
    "\n",
    "    return train_new, test_new\n",
    "\n",
    "\n",
    "encoded_column_train,encoded_column_test = hotone(food_train_no_ch['household_serving_common'],food_test_no_ch['household_serving_common'])\n",
    "\n",
    "\n",
    "train_encoded = pd.concat([food_train_no_ch, encoded_column_train], axis=1)\n",
    "test_encoded = pd.concat([food_test_no_ch, encoded_column_test], axis=1)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "final_train = pd.concat([train_encoded, train_ing], axis=1)\n",
    "final_test = pd.concat([test_encoded, test_ing], axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "train_y=final_train['category']\n",
    "train_predict_y=final_train['predict_y']\n",
    "final_train=final_train.drop(['category','brand','ingredients','serving_size_unit','household_serving_fulltext','ingredients_list','description','household_serving_common','top_ingredients','brand','serving_size','len_ingredients','predicted_category_by_brand','predicted_category','predict_y'],axis=1)\n",
    "final_test=final_test.drop(['brand','ingredients','household_serving_fulltext','ingredients_list','description','household_serving_common','top_ingredients','brand','serving_size', \"serving_size_unit\"],axis=1)\n",
    "final_train.columns = final_train.columns.astype(str)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lihikreiner/miniconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/lihikreiner/miniconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_n,test_n,train_y_n,test_y_n=train_test_split(final_train,train_y,train_size=0.80,random_state=100)\n",
    "clf = RandomForestClassifier(max_depth=45, random_state=0,n_estimators=2000)\n",
    "clf.fit(train_n,train_y_n)\n",
    "\n",
    "test_predict_forest=clf.predict(test_n)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8455361360415683"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_predict_forest==test_y_n).sum()/len(test_predict_forest)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the provided code snippet, our intention is to create a new model based on the 'predict_y' column.\n",
    "If there exists a value that is not None within this column, we will use those values.\n",
    " Alternatively, if no such non-None value is present, we will use the prediction value generated by the random forest model.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "for index in test_n.index:\n",
    "    if train_predict_y.loc[index] is None:\n",
    "        test_index = test_n.index.get_loc(index)\n",
    "        train_predict_y.loc[index] = test_predict_forest[test_index]\n",
    "\n",
    "train_predict_y = train_predict_y.loc[test_n.index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8548260116517084"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_predict_y==test_y_n).sum()/len(train_predict_y)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The achieved accuracy did not meet our expectations. Therefore, we have made the decision to proceed with another model.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lihikreiner/miniconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/lihikreiner/miniconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "final_test.columns = final_test.columns.astype(str)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=45, random_state=0,n_estimators=2000)\n",
    "clf.fit(final_train,train_y)\n",
    "test_predict_forest=clf.predict(final_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "np.savetxt('test_predict_RF.txt', test_predict_forest, fmt='%s')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "pd.Series(test_predict_forest, index=food_test['idx'], name='pred_cat').to_csv('model01.csv',index_label='idx')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
